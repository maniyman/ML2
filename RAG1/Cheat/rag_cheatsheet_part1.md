# ğŸ§  RAG Part 1 â€“ Cheat Sheet (LangChain + Chroma + Gemini)

## ğŸ“Œ Ãœberblick: Retrieval-Augmented Generation (RAG)

RAG kombiniert zwei Hauptkomponenten:
1. **Retrieval**: Relevante Textausschnitte aus externen Quellen suchen (z.â€¯B. ChromaDB).
2. **Generation**: Antwort mit einem LLM generieren (z.â€¯B. Google Gemini), basierend auf den gefundenen Informationen.

---

## ğŸ”§ Komponenten des RAG-Workflows

1. **Text-Splitting** â€“ Zerlegt lange Dokumente in Ã¼berlappende Chunks
2. **Embedding-Erzeugung** â€“ Wandelt Text in semantische Vektoren um
3. **Vektor-Datenbank** â€“ Speichert Embeddings zur spÃ¤teren semantischen Suche
4. **Retriever** â€“ Findet passende Textausschnitte zu einer User-Query
5. **LLM + Prompt Injection** â€“ Generiert die Antwort basierend auf gefundenem Kontext

---

## ğŸ§© Code mit ErklÃ¤rungen

```python
# Step 1: (Optional) Installation der Libraries
# !pip install langchain langchain-community langchain-google-genai

# Step 2â€“3: BenÃ¶tigte Module importieren
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain.chains import RetrievalQA
import os
```

```python
# Step 5: API Key fÃ¼r Google Gemini setzen (unbedingt echten Key verwenden)
os.environ["GOOGLE_API_KEY"] = "your_api_key"
```

```python
# Step 7: PDF laden
loader = PyPDFLoader("2023_MicroLED_Pharma_EN_vf.pdf")
pages = loader.load()
```

```python
# Step 8: Erste Seite anzeigen (zur Kontrolle)
pages[0]
```

```python
# Step 10: Text in Ã¼berlappende Chunks aufteilen
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = text_splitter.split_documents(pages)
```

```python
# Step 11: Anzahl der Chunks anzeigen
len(texts)
```

```python
# Step 13: Embeddings erzeugen und in Chroma speichern
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
db = Chroma.from_documents(texts, embeddings)
```

```python
# Step 15: Retriever erstellen
retriever = db.as_retriever()
```

```python
# Step 17: RetrievalQA-Chain aufbauen (Retriever + LLM + Prompt-Injection)
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatGoogleGenerativeAI(model="gemini-pro"),
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True
)
```

```python
# Step 19: Frage stellen (Query)
query = "What is microLED and what is it used for?"
result = qa_chain(query)
```

```python
# Step 21: Antwort anzeigen
print(result["result"])

# Optional: Quellen anzeigen
# result["source_documents"]
```

---

## âœ… Lessons Learned

- Text-Chunks mit Overlap bewahren Kontext
- Embeddings machen Texte maschinell â€vergleichbarâ€œ
- Vektor-Datenbanken sind essenziell fÃ¼r semantisches Retrieval
- Prompt-Injection ersetzt Fine-Tuning
- RAG reduziert Halluzinationen und erhÃ¶ht Relevanz

---

## ğŸ§  Typische Fehler

- Kein Overlap beim Chunking â†’ Kontextverlust
- Falsches Embedding-Modell â†’ schlechte Retrieval-Ergebnisse
- chain_type="stuff" bei groÃŸen Mengen unpraktisch (Kontextlimit!)
- Fehlender API-Key oder falscher Dateipfad

---

## ğŸ›  Tools & Libraries

- LangChain (Chain-Management)
- Chroma (Vektor-Datenbank)
- Google Generative AI (Embeddings + Chat)
- PDFLoader & TextSplitter (Preprocessing)

---

## ğŸ” NÃ¤chste Schritte

- Retrieval-Optimierung (Top-k, Filter, Hybrid-AnsÃ¤tze)
- Vergleich: `stuff` vs `map_reduce` Chain-Typ
- Erweiterung auf andere Dateiformate
- Anbindung an UI oder API-Endpunkt
