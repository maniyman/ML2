
# ğŸ—ï¸ Multimodal AI Project â€” Week 10 (ML2 FS2025)

This repository contains notebooks and exercises for **Machine Learning II (Week 10)** on Multimodal AI.

## ğŸ“‚ Contents

- `SW10_E1_VLM_Basics.ipynb`: Vision-Language Models basics
- `SW10_E2_Identify_Obj_Positions_startingpoint.ipynb`: Object detection comparison (YOLO, GPT-4, Gemini)
- `SW10_HandsOn_Object_Detection_Basics_YOLO_OWL_ViT.ipynb`: YOLO and OWL-ViT object detection

## ğŸš€ How to Run

1ï¸âƒ£ Clone the repo  
```bash
git clone https://github.com/zhaw-iwi/MultimodalInteraction_ObjDet.git
```

2ï¸âƒ£ Install dependencies  
```bash
pip install -r requirements.txt
```

3ï¸âƒ£ Load `.env` file for API keys  
```bash
from dotenv import load_dotenv
load_dotenv()
```

4ï¸âƒ£ Run notebooks (locally, Colab, LightningAI, or Codespaces)

---

## ğŸ’¡ Main Tasks

âœ… Run VLM models (CLIP, OWL-ViT)  
âœ… Run YOLO for object detection  
âœ… Compare results across methods  
âœ… Integrate multimodal inputs (text, image, audio)

---

## ğŸ“¦ Dependencies

- `torch`
- `transformers`
- `ultralytics`
- `opencv-python`
- `python-dotenv`

---

## ğŸ›¡ï¸ Best Practices

- Use `.env` for sensitive keys  
- Check CUDA/GPU support  
- Validate input shapes  
- Set random seeds for reproducibility

---

## ğŸ“ Contact

Maintainer: Dr. Elena Gavagnin  
Email: elena.gavagnin@zhaw.ch
