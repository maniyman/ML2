
# ğŸ“˜ Multimodal AI â€“ Modelle laden, anwenden, interpretieren und kombinieren  
**ML2 â€“ Woche 10: YOLO, Gemini, GPT-4V**

---

## ğŸ§  Ziel dieses Sheets

Dieses Sheet zeigt dir:
- âœ… Wie du die Modelle **lÃ¤dst und verwendest**
- âœ… Welche **Tools fÃ¼r welche Aufgaben** geeignet sind
- âœ… Wie du die **Ergebnisse interpretierst und kombinierst**

Kein Training, keine Theorie â€“ nur praxisnahe Anwendung.

---

## ğŸ“Œ 1. YOLO â€“ Objekterkennung mit Bounding Boxes

### âœ… Was ist das?
YOLO erkennt Objekte in Bildern oder Videos und gibt dir **exakte Koordinaten + Labels**.

### ğŸ› ï¸ So benutzt du YOLO:

```python
# Installiere YOLOv8
!pip install ultralytics

# Modell laden
from ultralytics import YOLO
model = YOLO("yolov8n.pt")  # Modell auswÃ¤hlen

# Bild analysieren
results = model("DEIN_BILD.jpg")  # <-- Passe den Bildpfad an

# Ergebnisse anzeigen
results[0].show()

# Labels + Konfidenzwerte ausgeben
for box in results[0].boxes:
    cls_id = int(box.cls[0])
    label = results[0].names[cls_id]
    conf = box.conf[0].item()
    print(f"{label}: {conf:.2f}")
```

ğŸ“Œ **Nutzen:**  
- Ideal, wenn du wissen willst **wo genau etwas ist** (z.â€¯B. â€Tasse bei x=124, y=88â€œ)

---

## ğŸ“Œ 2. Gemini â€“ Bildkontext verstehen (Google)

### âœ… Was ist das?
Gemini erkennt, **was auf einem Bild passiert** (z.â€¯B. â€Person liegt am Bodenâ€œ, â€etwas sieht gefÃ¤hrlich ausâ€œ).

### ğŸ› ï¸ Anwendung:

Du gibst Gemini ein Bild + Prompt wie:
> â€Welche Objekte befinden sich auf dem Tisch? Wer wirkt verletzt? Was passiert in der Szene?â€œ

ğŸ“Œ **Ergebnis:**
Text mit semantischer Interpretation.  
Kein Code nÃ¶tig â€“ du nutzt Gemini Ã¼ber Web-Interface oder API.

ğŸ“Œ **Nutzen:**
- Ideal, wenn du verstehen willst, **was die Szene bedeutet**
- Kombinierbar mit YOLO: Gemini sagt â€wofÃ¼râ€œ, YOLO sagt â€woâ€œ

---

## ğŸ“Œ 3. OpenAI GPT-4 mit Vision â€“ Sprachlich strukturierte Bildanalyse

### âœ… Was ist das?
GPT-4V versteht Bilder und beschreibt sie in Textform â€“ wie Gemini, aber vorsichtiger.

### ğŸ› ï¸ Anwendung (via Webinterface / API):

> â€Dieses Bild zeigt eine Unfallszene. Welche Personen brauchen Hilfe? Beschreibe jede Person.â€œ

ğŸ“Œ **Nutzen:**
- FÃ¼r strukturierte Entscheidungslogik (z.â€¯B. Triage)
- Kein Bounding Box Output, aber gute Beschreibung

---

## ğŸ”€ Kombination der Tools: Was benutze ich wann?

| Aufgabe | Modell |
|--------|--------|
| ğŸ¯ Objekt exakt erkennen | **YOLO** |
| ğŸ§  Szene beschreiben & deuten | **Gemini / GPT-4V** |
| ğŸ§© Semantische Auswahl treffen (z.â€¯B. 'wichtigstes Objekt') | **GPT / Gemini** |
| ğŸ—£ï¸ Text in Sprache umwandeln | **gTTS / pyttsx3** |
| ğŸ¤– Gesamtsystem aufbauen | **YOLO + GPT/Gemini kombiniert** |

---

## ğŸ§  Ergebnisse interpretieren

### YOLO:
- Gibt dir `label`, `confidence`, `xyxy` (Koordinaten)
- Du weiÃŸt genau, wo das Objekt im Bild ist
- Nutze es fÃ¼r Greifroboter, Tracking, Bounding Box Visualisierung

### Gemini / GPT:
- Gibt dir Text â†’ â€Person liegt bewusstlos links am Bodenâ€œ
- Du weiÃŸt, **was das Objekt bedeutet**
- Nutze es fÃ¼r Entscheidungslogik, Sprachassistenz, Kontextverarbeitung

---

## âœ… Best Practice: Kombiniere die StÃ¤rken

Beispiel:
- ğŸ§  Gemini sagt: â€Die Tasse neben dem Messer ist gefÃ¤hrlich.â€œ
- ğŸ“¦ YOLO erkennt: Es gibt 3 Tassen, 1 Messer â†’ du findest die richtige Tasse per Position

---

## ğŸ§© Fazit

> Nutze YOLO fÃ¼r prÃ¤zise Koordinaten.  
> Nutze GPT oder Gemini fÃ¼r Bedeutung und Kontext.  
> Kombiniere beide â€“ dann bist du bereit fÃ¼r deinen Hackathon.
