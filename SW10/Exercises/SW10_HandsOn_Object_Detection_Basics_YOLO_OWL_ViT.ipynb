{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmX7B7bOKrcs"
      },
      "source": [
        "# Object Detection  - YOLO & OWL-ViT\n",
        "This tutorial demonstrates how to use YOLO (You Only Look Once) from the [Ultralytics](https://github.com/ultralytics/yolov5) library for object detection. It includes steps for:\n",
        "\n",
        "- Running object detection inference on images/videos\n",
        "- Fine-tuning YOLO for custom datasets\n",
        "- Comparing YOLO with OWl-VIT for zero-shot learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzFABV55Krcu"
      },
      "source": [
        "## 1. Perform Object Detection Inference\n",
        "First thing We'll use YOLOv8 from Ultralyics for object detection on a sample image.\n",
        "We aim to utilize the pre-trained YOLOv8 model to detect objects in a sample image. This involves loading the model, providing an image for input, and interpreting the model's predictions.\n",
        "\n",
        "**Key Concepts:**\n",
        "- **Inference**: The process of using a trained model to make predictions on new data.\n",
        "- **YOLOv8**: A state-of-the-art version of the YOLO (You Only Look Once) architecture, known for its speed and accuracy in object detection tasks.\n",
        "\n",
        "**Steps:**\n",
        "1. Load the YOLOv8 model using the Ultralytics library.\n",
        "2. Perform inference on a sample image to detect objects.\n",
        "3. Visualize the results, including bounding boxes and class labels.\n",
        "\n",
        "**Support Material:**\n",
        "- https://docs.ultralytics.com/models/yolov8/\n",
        "- https://docs.ultralytics.com/tasks/detect/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "sus6DopNA8wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "0i0hLmOE-b3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/ML_FS25/Week10/\""
      ],
      "metadata": {
        "id": "2ilrQDi-ChfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubtWc477Krcu"
      },
      "outputs": [],
      "source": [
        "# Import YOLO and load a pre-trained model\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "\n",
        "# Load the YOLOv8 pre-trained model\n",
        "model = YOLO('yolov8n.pt')  # nano model for quick inference\n",
        "\n",
        "# Run inference on a sample image\n",
        "\n",
        "results = model(dir+'images/street_scene.jpg', save = True)  # Displays image with detections\n",
        "\n",
        "for result in results:\n",
        "    print(result.boxes)  # Boxes object for bounding box outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oR6zbFwpKrcv"
      },
      "source": [
        "## 2. Fine-Tuning YOLO on Custom Dataset\n",
        "Fine-tuning YOLO requires a dataset formatted in the YOLO format. We'll use a small public dataset for demonstration.\n",
        "We will adapt the pre-trained YOLO model to a custom dataset. This process, known as fine-tuning, enables YOLO to specialize in detecting specific objects not included in its original training.\n",
        "\n",
        "**Key Concepts:**\n",
        "- **Fine-tuning**: Adapting a pre-trained model to new data by continuing the training process.\n",
        "- **Custom Dataset**: A dataset that contains specific objects relevant to a new application, different from those YOLO was trained on (e.g. https://docs.ultralytics.com/datasets/detect/signature/.)\n",
        "\n",
        "**Steps:**\n",
        "1. Prepare the custom dataset by organizing images and labels in the required format.\n",
        "2. Configure the YOLO training pipeline.\n",
        "3. Train the model and evaluate its performance.\n",
        "\n",
        "**Support Material:**\n",
        "- https://docs.ultralytics.com/modes/train/\n",
        "- https://docs.ultralytics.com/modes/val/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7tCEMh6Krcv"
      },
      "outputs": [],
      "source": [
        "# Download a sample dataset (e.g., Signature)\n",
        "!wget -q https://github.com/ultralytics/assets/releases/download/v0.0.0/signature.zip\n",
        "!unzip -q signature.zip -d ./datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGvTet3cKrcv"
      },
      "outputs": [],
      "source": [
        "# Train YOLO on the dataset\n",
        "results = model.train(data='./datasets/signature.yaml', epochs=10, imgsz=640, batch=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj3pGSIMKrcv"
      },
      "outputs": [],
      "source": [
        "model = YOLO(\"runs/detect/train12/weights/best.pt\")  # load a custom model, check the path depending on your output before!!\n",
        "\n",
        "# Predict with the model\n",
        "results = model.predict(\"images/example_signature.jpg\", conf=0.75) #check params if you need to improve detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyDFiWMWKrcv"
      },
      "source": [
        "## 3. Zero-Shot Learning with OWL-ViT\n",
        "Switch to `OWL-ViT` to see how it performs with zero-shot learning capabilities. Zero-shot means detecting objects without prior specific training.\n",
        "\n",
        "OWL-ViT (Open Vocabulary Learning with Vision Transformers) is a cutting-edge model designed for open vocabulary object detection. Unlike traditional models, OWL-ViT combines vision transformers with text embeddings, enabling it to:\\n\\n\n",
        "- Understand textual descriptions of objects, even if it hasn't seen them during training.\n",
        "- Detect and classify objects based on descriptive input, making it suitable for diverse applications.\n",
        "- Perform zero-shot learning by generalizing to new object classes without additional training.\\n\\n\"\n",
        "\n",
        "**Steps in Using OWL-ViT:**\n",
        "1. Model Initialization**: Set up the OWL-ViT model.\n",
        "2. Text Input for Object Descriptions: Provide descriptive prompts (e.g., 'a red car' or 'a black cat to guide detection.\n",
        "3. Inference and Visualization: Process an image or video, detect objects based on text descriptions and visualize results.\\n\\n\"\n",
        "\n",
        "OWL-ViT excels in scenarios where predefined object classes are insufficient, such as detecting rare or domain-specific objects.\n",
        "\n",
        "**Support Material**:\n",
        "- https://huggingface.co/docs/transformers/en/model_doc/owlvit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awTNwmp3Krcv"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patheffects as pe\n",
        "\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "image = Image.open(dir+\"images/street_scene.jpg\")\n",
        "\n",
        "\n",
        "def preprocess_outputs(output):\n",
        "    input_scores = [x[\"score\"] for x in output]\n",
        "    input_labels = [x[\"label\"] for x in output]\n",
        "    input_boxes = []\n",
        "    for i in range(len(output)):\n",
        "        input_boxes.append([*output[i][\"box\"].values()])\n",
        "    input_boxes = [input_boxes]\n",
        "    return input_scores, input_labels, input_boxes\n",
        "\n",
        "\n",
        "def show_box(box, ax):\n",
        "    x0, y0 = box[0], box[1]\n",
        "    w, h = box[2] - box[0], box[3] - box[1]\n",
        "    ax.add_patch(\n",
        "        plt.Rectangle((x0, y0), w, h, edgecolor=\"green\", facecolor=(0, 0, 0, 0), lw=2)\n",
        "    )\n",
        "\n",
        "\n",
        "def show_boxes_and_labels_on_image(raw_image, boxes, labels, scores):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(raw_image)\n",
        "    for i, box in enumerate(boxes):\n",
        "        show_box(box, plt.gca())\n",
        "        plt.text(\n",
        "            x=box[0],\n",
        "            y=box[1] - 12,\n",
        "            s=f\"{labels[i]}: {scores[i]:,.4f}\",\n",
        "            c=\"beige\",\n",
        "            path_effects=[pe.withStroke(linewidth=4, foreground=\"darkgreen\")],\n",
        "        )\n",
        "    plt.axis(\"on\")\n",
        "    plt.show()\n",
        "\n",
        "OWL_checkpoint = \"google/owlvit-base-patch32\"\n",
        "\n",
        "text = [\"a person on the floor\", \"a church \"]\n",
        "\n",
        "# Load the model\n",
        "detector = pipeline(\n",
        "    model= OWL_checkpoint,\n",
        "    task=\"zero-shot-object-detection\"\n",
        ")\n",
        "\n",
        "output = detector(\n",
        "    image,\n",
        "    candidate_labels = text\n",
        ")\n",
        "\n",
        "print(output)\n",
        "\n",
        "input_scores, input_labels, input_boxes = preprocess_outputs(output)\n",
        "\n",
        "# Show the image with the bounding boxes\n",
        "show_boxes_and_labels_on_image(\n",
        "    image,\n",
        "    input_boxes[0],\n",
        "    input_labels,\n",
        "    input_scores\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}