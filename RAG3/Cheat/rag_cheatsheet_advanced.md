
# ğŸ§  RAG Cheat Sheet â€“ Advanced Concepts & Best Practices

## ğŸ” Zentrale Konzepte & Begriffe

### ğŸ”¸ Cosine Similarity
- Misst den Winkel zwischen zwei Vektoren im Vektorraum.
- Wertebereich: `[-1, 1]` â€“ je nÃ¤her an `1`, desto Ã¤hnlicher.
- Wichtig fÃ¼r die Ã„hnlichkeitsbewertung bei Embeddings.

### ğŸ”¸ Vector Stores
- Speichern unstrukturierte Daten als Vektoren.
- Retrieval erfolgt Ã¼ber Ã„hnlichkeitsmetriken (z.â€¯B. Cosine).
- Beispiele: FAISS, Chroma, Weaviate, Pinecone.

### ğŸ”¸ Indexierung in Vector Stores
- Einsatz von **ANN (Approximate Nearest Neighbor)** zur Effizienzsteigerung.
- ANN-Techniken:
  - FAISS (Clustering)
  - HNSW (Graph-basiert)
  - ANNOY (Tree-basiert)
  - LSH (Hashing)
  - PQ (Quantisierung)

### ğŸ”¸ Hybrid Search
- Kombination aus Vektor-Suche (semantisch) und Keyword-Suche (symbolisch).
- Verbessert Relevanz durch parallele Strategien.

### ğŸ”¸ Distance Thresholding
- Legt maximale Distanz zwischen Query und Dokument fest.
- Nutzt Kombination mit Top-k Retrieval.

### ğŸ”¸ Prompt Design
- Strukturiere Prompts mit Tags wie `<information>` fÃ¼r klare Trennung.
- Position der Informationen im Prompt (Anfang/Ende) beeinflusst Gewichtung durch das LLM.

---

## ğŸ› ï¸ Tools, Libraries & Architekturen

| Kategorie          | Beispiele / Hinweise                              |
|--------------------|----------------------------------------------------|
| Vektor-Datenbanken | FAISS, Chroma, Pinecone, Weaviate                  |
| Frameworks         | LangChain, LlamaIndex, Haystack                    |
| Embeddings         | OpenAI, HuggingFace, SentenceTransformers         |
| Evaluation         | GPT-4 fÃ¼r automatische Antwortbewertung           |

---

## ğŸ§ª Query Transformation Techniken

| Methode                       | Zweck                                            |
|------------------------------|--------------------------------------------------|
| Query Expansion              | FÃ¼ge hypothetische Antwort in die Query ein     |
| Query Augmentation / Rewrite| Lass LLM alternative, bessere Queries generieren |
| HyDe                         | Erzeuge hypothetische Dokumente zur Retrieval-Verbesserung |
| Subquestions / Planning      | Teile komplexe Fragen in Subfragen auf          |
| Multi-Step Reasoning         | Iterative Subfrage-Generierung durch das LLM    |

---

## ğŸ§  Lessons Learned & typische Fehler

- âŒ *Falsche Ã„hnlichkeitsmetrik*: nicht jede Aufgabe passt zu Cosine Similarity.
- âŒ *Zu niedrige Top-k Werte*: relevante Kontexte kÃ¶nnen Ã¼bersehen werden.
- âŒ *Unklare Promptstruktur*: beeintrÃ¤chtigt Modellverhalten.
- âœ… *Query Transformation steigert Recall signifikant*.
- âœ… *Reranking via Cross-Encoder verbessert Precision*.

---

## ğŸ§¾ Wichtige CodeblÃ¶cke aus dem Notebook `advanced_rag.ipynb`

```python
import tqdm
import glob
from PyPDF2 import PdfReader
import re
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
```

```python
### load the pdf from the path
glob_path = "data/**/*.pdf"
pdf_paths = glob.glob(glob_path, recursive=True)
pdf_texts = []

for path in pdf_paths:
    reader = PdfReader(path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    pdf_texts.append(text)
```

```python
# Create a splitter: 2000 characters per chunk
chunks = []
for text in pdf_texts:
    for i in range(0, len(text), 2000):
        chunk = text[i:i+2000]
        chunks.append(chunk)
```

```python
model_name = "paraphrase-multilingual-MiniLM-L12-v2"
model = SentenceTransformer(model_name)
embeddings = model.encode(chunks)
```

```python
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

query = "What are the side effects of the drug?"
query_embedding = model.encode([query])
D, I = index.search(query_embedding, k=5)
retrieved_chunks = [chunks[i] for i in I[0]]
```

---

## ğŸ›  Hinweise zur praktischen RAG-Umsetzung

- **Datenvorverarbeitung**:
  - Chunking (z.â€¯B. nach AbsÃ¤tzen oder Tokens)
  - Metadaten anreichern (z.â€¯B. Quelle, Zeitstempel)

- **Embedding-Strategie**:
  - Einheitliche Modelle fÃ¼r Query & Dokumente verwenden
  - Spezialisierte Embedding-Modelle fÃ¼r FachdomÃ¤nen nutzen

- **Retriever-Tuning**:
  - Top-k und Schwellenwerte experimentell bestimmen
  - Hybrid & MMR-Strategien testen
  - Optional: Benutzerfeedback zur Relevanz nutzen

---

## ğŸ“Œ Quellen & Referenzen

- [Weaviate Blog: Why Vector Search is Fast](https://weaviate.io/blog/why-is-vector-search-so-fast)
- [Pinecone Tutorials](https://www.pinecone.io/learn/)
- [Towards Data Science RAG Series](https://towardsdatascience.com/tagged/rag)
